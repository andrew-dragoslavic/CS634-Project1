{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Andrew Dragoslavic\n",
    "\n",
    "NJIT UCID: amd83\n",
    "\n",
    "Email Address: amd83@njit.edu\n",
    "\n",
    "10/10/2024\n",
    "\n",
    "Professor: Yasser Abduallah\n",
    "\n",
    "CS 634 101 Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Abstract**:\n",
    "In this project I looked into Apriori Algorithm, this is a basic concept within data mining and explores the use of association rules in order to find common patterns within transactions. By implementing the model with the use of data mining tools and concepts I was able to analyze the effectiveness of the custom Apriori model and get comparisons to built in python packages and see the insights they are able to provide regarding transaction patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction**:\n",
    "\n",
    "Data mining is an integral part of modern day data science and looks into finding patterns and association rules within large amounts of data. This project in specifically looks into Association Rule Mining to find patterns in transactional datasets and more specifically implements the Apriori Algorithm and its importance within industry. In this report we will take a deep dive into the algorithm and the underlying concepts being used.\n",
    "\n",
    "The goal of the Apriori Algorithm is to find association rules within a set of transactions by finding which items show up the most and surpass a certain threshold. This threshold was given by the user input support and confidence. From there we looked at the count of each item and whichever fall below the threshold get removed from the list. This is a brute force method to find frequent items within a large dataset and generate association rules within the data. This is done by starting with 1 itemsets through k itemsets and looking at how many times the subset shows up within the data and removing those subest that are below the defined threshold. \n",
    "\n",
    "In the code below I implemented a custom Apriori Algorithm to multiple custom datasets based on different stores and looked to find frequent itemsets and association rules using the following steps\n",
    "* Load the datasets from CSV files and reformat the transactions list to remove any inconsistentcies\n",
    "* Parsing the data from the CSV files to get a count of the number of each of the items in the list\n",
    "* Comparing this initial candidate list with the input support and generating the next candidate itemset based on which items remain\n",
    "* Once all the frequent itemsets are found calculate the confidence of each and compare to the user input confidence and generate rules for those above the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Core Concepts**\n",
    "* **Frequent Itemsets**\n",
    "  * The Apriori Algorithm looks into finding frequent itemsets within the data which is items that occur within the same transaction. It does this by checking how many transactions a subset occurs in and depending on the support and confidence will be labeled as a frequent itemset. This is very useful in retail to find items commonly bought together and customer purchase patterns.\n",
    "\n",
    "* **Support and Confidence**\n",
    "  * Support and Confidence are user set variables in the Apriori Algorithm where support measure how often a given itemset shows up within the entire dataset and confidence is measure showing how likely two items are to be pruchased together\n",
    "\n",
    "* **Generating Assocation Rules**\n",
    "  * By generating the assocaiton rules from the frequent itemesets found we are able to determine the likeliness that purchasing one item will lead to buying the other item which helps in industry with customers behaviors and purchase patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Project Outline**\n",
    "\n",
    "* **Data Loading and Preprocessing**\n",
    "  * We began by getting transaction data from different stores and using this as our itemset data. Each transaction consisted of a list of different items that were purchased together. This data was preprocessed so that it could be used properly and identified correctly within the algorithm. This was done by formatting each transaction into a set and removing and unnecessary white space.\n",
    "* **Minimum Support and Confidence**\n",
    "  * The support and confidence are user defined input each time the algorithm is run and is used to remove patterns that are decided to be not relevant enough to keep for later stage of the algorithm\n",
    "* **Candidate Itemsets**\n",
    "  * The base of the Apriori Algorithm revolves around generating candidate itemsets and seeing if any of them occur enough to surpass the threshold set by the support. Starting at the 1-itemsets and keeping whichever occur enough we then move on 2-itemsets which are made by running combinations on the remaining items from the previous candidates. We continue this process until there are no longer any k-itemsets that are above the support threshold.\n",
    "* **Support and Confidence Calculations**\n",
    "  * **Support**\n",
    "    * The support for each itemset from the candidates is found by getting a count of how many times each given itemset shows up throughout all the transasctions. Once this is found it is compared with the minimum threshold and either kept or discarded.\n",
    "  * **Confidence**\n",
    "    * The confidence is used to show us the likelihood that two items are purcahsed together and the strength of the two items association. This is calculated by using the support of the joint itemset and the support of it's individual items.\n",
    "* **Association Rules**\n",
    "  * Items that are above both the support and confidence threshold are taken into account and are output at the end of execution to determine which sets are most likely to be bought together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**\n",
    "\n",
    "From this project I was able to look into the inner workings of the Apriori Algorithm and see how association rules work in depth with transactional data. I was able to develop my own version of the Apriori Algorithm and see how successful it was in determining the most common itemsets. In addition to this we were able to look into the effect that support and confidence have on the outputs of the generated association rules. Based on how high or low the support and confidence were set to had a big effect on what the frequent itemsets were and how many association rules were generated. Lastly I was able to compare this to built in Python packages which allow the use of the Apriori algorithm and Frequent Pattern Tree algorithm and ensure that my algorithm was able to generate the same results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**\n",
    "\n",
    "In conclusion this project looks into the data mining concepts relating to Apriori Algorithm and also compares it with the Frequent Pattern Tree Algorithm. From this we were able to look into the algorithm in a step by step approach and attempt to implement it in the most efficient and effective way possible. We were able to see the steps that go into extracting association rules including preprocessing, support, confidence, and candidate itemsets. By being able to develop our own brute force approach we were able to see how the algorithm works at each stage and understand one of the core concept of data mining by getting a hands on experience with association rules and understand how it is used at an industry level with transactional data and see the meaningful patterns and helpful information that can be extracted from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import combinations for making the candidate itemsets and pandas to load in the the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to set function responsible for making each list of items into a set and separating them by the commas and removing any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToSet(transactions):\n",
    "    return set(item.strip() for item in transactions.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the csv file with the transactions and items and apply the function and then specify the support and confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(\"5/Transactions.csv\")\n",
    "transactions[\"Transaction\"] = transactions[\"Transaction\"].apply(convertToSet)\n",
    "itemset = pd.read_csv(\"5/Items.csv\")\n",
    "support = 50\n",
    "confidence = 80\n",
    "threshold = (support / 100) * len(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of all the items from the initial itemset and use this to make the k-itemsets. For each candidate itemset go through the transactions and see if that subset exist and if so get a sum of it and add it to the itemset dictionary. Once all the combinations have been exhausted and summed up, whichever are below the threshold remove from the itemset dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateItemsets(prevItemsets, transactions, threshold, k):\n",
    "    itemsets = {}\n",
    "    itemList = list(prevItemsets.keys())\n",
    "\n",
    "    for combo in combinations(itemList, k):\n",
    "        combo = set(combo)\n",
    "        count = sum(\n",
    "            1\n",
    "            for transaction in transactions[\"Transaction\"]\n",
    "            if combo.issubset(transaction)\n",
    "        )\n",
    "        if count >= threshold:\n",
    "            itemsets[tuple(combo)] = count\n",
    "    return itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the rules we go through the keys of the itemset dictionary being the items in it and check if the length of it is more than one. If not we ignore it since there is no rule to make for it. If it is more tho we run a for loop from 1 to the length of the itemset where the current i determines the combination being made and whatever combinations are made from that we make the start of the rule and to get the other part of the rule we subtract the combo from the itemset. Once we have this we then get the support for the start part of the rule by again going through the transactions and finding where it appears and we do this for the itemset as a whole as well. This is done for us to be able to calculate the confidence of each association rule. Once we have the supports we compare the calculated confidence to the minimum required and if it meets the requiremnt we output the rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRules(itemsets, transactions, min_support, min_confidence):\n",
    "    count = 0\n",
    "    # Go through the final dictionary and if the length is only one item ignore it\n",
    "    # If the length is more than two generate all the possible combinations\n",
    "    for itemset in itemsets.keys():\n",
    "        k = len(itemset)\n",
    "        if k < 2:\n",
    "            continue\n",
    "        for i in range(1, k):\n",
    "            for combo in combinations(itemset, i):\n",
    "                start = set(combo)\n",
    "                itemset = set(itemset)\n",
    "                res = itemset - start\n",
    "                start_support = sum(\n",
    "                    1\n",
    "                    for transaction in transactions[\"Transaction\"]\n",
    "                    if start.issubset(transaction)\n",
    "                )\n",
    "                rule_support = sum(\n",
    "                    1\n",
    "                    for transaction in transactions[\"Transaction\"]\n",
    "                    if itemset.issubset(transaction)\n",
    "                )\n",
    "                confidence = rule_support / start_support\n",
    "                if confidence >= min_confidence / 100:\n",
    "                    count += 1\n",
    "                    print(\n",
    "                        f\"\\nRule {count}: {', '.join(start)} -> {', '.join(res)}, Confidence: {confidence}\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to generate the frequent itemsets we then had to take the resulting dictionary and create a string to hold them all. We did this by going one itemset at a time and appending them to a list. Once we had a list filled with all the itemsets we then iterated through the list in order to construct the string by putting each item in curly braces and separating them with commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFrequentItems(itemsets):\n",
    "    freq = []\n",
    "    freqString = \"\"\"\\nFrequent Items: \"\"\"\n",
    "    for itemset in itemsets:\n",
    "        freq.append(itemset)\n",
    "    for i in range(len(freq)):\n",
    "        freqString += \"{\" + \", \".join(freq[i]) + \"}\"\n",
    "        if i < len(freq) - 1:\n",
    "            freqString += \", \"\n",
    "    return freqString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly in the apriori function we go through the transactions and get a count of each of the items. Once we have that we then compare this with the threshold to see what items are left. With these remaining items we then run the generateItemsets function to get other possible k-itemsets and add them to the results dictionary. And after we have found all valid itemsets we then use them to call the generateRules function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(transactions, min_support, min_confidence):\n",
    "    itemsets = {}\n",
    "    results = {}\n",
    "\n",
    "    for index, row in transactions.iterrows():\n",
    "        for item in row[\"Transaction\"]:\n",
    "            itemsets[item] = itemsets.get(item, 0) + 1\n",
    "\n",
    "    threshold = (min_support / 100) * len(transactions)\n",
    "    itemsets = {k: v for k, v in itemsets.items() if v >= threshold}\n",
    "    # results.update(itemsets)\n",
    "    k = 1\n",
    "\n",
    "    while True:\n",
    "        new = generateItemsets(itemsets, transactions, threshold, k)\n",
    "        if not new:\n",
    "            break\n",
    "        results.update(new)\n",
    "        k += 1\n",
    "    generateRules(results, transactions, min_support, min_confidence)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the results from the Apriori function we save them into results and use this to run the generateFrequentItems function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rule 1: Modern Pants -> Sweatshirts, Confidence: 1.0\n",
      "\n",
      "Rule 2: Sweatshirts -> Running Shoe, Confidence: 0.8461538461538461\n",
      "\n",
      "Rule 3: Socks -> Running Shoe, Confidence: 0.8461538461538461\n",
      "\n",
      "Rule 4: Sweatshirts -> Socks, Confidence: 0.9230769230769231\n",
      "\n",
      "Rule 5: Socks -> Sweatshirts, Confidence: 0.9230769230769231\n",
      "\n",
      "Rule 6: Rash Guard -> Swimming Shirt, Confidence: 0.8333333333333334\n",
      "\n",
      "Rule 7: Swimming Shirt -> Rash Guard, Confidence: 0.9090909090909091\n",
      "\n",
      "Rule 8: Running Shoe, Sweatshirts -> Socks, Confidence: 0.9090909090909091\n",
      "\n",
      "Rule 9: Running Shoe, Socks -> Sweatshirts, Confidence: 0.9090909090909091\n",
      "\n",
      "Rule 10: Sweatshirts, Socks -> Running Shoe, Confidence: 0.8333333333333334\n",
      "\n",
      "Frequent Items: {Modern Pants}, {Running Shoe}, {Sweatshirts}, {Socks}, {Rash Guard}, {Swimming Shirt}, {Modern Pants, Sweatshirts}, {Running Shoe, Sweatshirts}, {Running Shoe, Socks}, {Sweatshirts, Socks}, {Rash Guard, Swimming Shirt}, {Running Shoe, Sweatshirts, Socks}\n"
     ]
    }
   ],
   "source": [
    "results = apriori(transactions, support, confidence)\n",
    "print(generateFrequentItems(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compare the results from our brute force approach by using built in packages for the Apriori and Frequent Pattern Tree Algorithms. The first step was to reformat the data into a 2D array so that it can be read by the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('5/Items.csv')\n",
    "transactions = pd.read_csv('5/Transactions.csv')\n",
    "itemsList = []\n",
    "for index, row in transactions.iterrows():\n",
    "    itemSet = [item.strip() for item in row['Transaction'].split(',')]\n",
    "    itemsList.append(itemSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data was properly formatted we then intalled the need packages from mlxtend such as apriori, association rules, fpgrowth, and TransactionEncoder. With this we initialized an encoder and fit the itemlist into a dataframe to be read by the algorithms. Once this was then we ran the function on both and extracted the necessary information being the antecedent, consequents, and confidence for each of the rules and we output them to show that the brute force approach gets the same results as the built in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1: [frozenset({'Modern Pants'}), frozenset({'Sweatshirts'}), 1.0]\n",
      "\n",
      "Rule 2: [frozenset({'Rash Guard'}), frozenset({'Swimming Shirt'}), 0.8333333333333334]\n",
      "\n",
      "Rule 3: [frozenset({'Swimming Shirt'}), frozenset({'Rash Guard'}), 0.9090909090909091]\n",
      "\n",
      "Rule 4: [frozenset({'Socks'}), frozenset({'Running Shoe'}), 0.8461538461538461]\n",
      "\n",
      "Rule 5: [frozenset({'Sweatshirts'}), frozenset({'Running Shoe'}), 0.8461538461538461]\n",
      "\n",
      "Rule 6: [frozenset({'Sweatshirts'}), frozenset({'Socks'}), 0.923076923076923]\n",
      "\n",
      "Rule 7: [frozenset({'Socks'}), frozenset({'Sweatshirts'}), 0.923076923076923]\n",
      "\n",
      "Rule 8: [frozenset({'Running Shoe', 'Sweatshirts'}), frozenset({'Socks'}), 0.9090909090909091]\n",
      "\n",
      "Rule 9: [frozenset({'Running Shoe', 'Socks'}), frozenset({'Sweatshirts'}), 0.9090909090909091]\n",
      "\n",
      "Rule 10: [frozenset({'Sweatshirts', 'Socks'}), frozenset({'Running Shoe'}), 0.8333333333333334]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "encoder = TransactionEncoder()\n",
    "onehot = encoder.fit(itemsList).transform(itemsList)\n",
    "df = pd.DataFrame(onehot, columns = encoder.columns_)\n",
    "\n",
    "aprioriItemsets = apriori(df, min_support=0.50, use_colnames=True)\n",
    "fpgrowthItemsets = fpgrowth(df, min_support=0.50, use_colnames=True)\n",
    "\n",
    "# Generate the association rules\n",
    "aprioriRules = association_rules(aprioriItemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "fpgrowthRules = association_rules(fpgrowthItemsets, metric=\"confidence\", min_threshold = 0.8)\n",
    "for index, row in aprioriRules.iterrows():\n",
    "    antecedent_str = row['antecedents']\n",
    "    consequent_str = row['consequents']\n",
    "    confidence_str = row['confidence']\n",
    "    \n",
    "    print(f\"Rule {index + 1}: [{antecedent_str}, {consequent_str}, {confidence_str}]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1: [frozenset({'Sweatshirts'}), frozenset({'Running Shoe'}), 0.8461538461538461]\n",
      "\n",
      "Rule 2: [frozenset({'Sweatshirts'}), frozenset({'Socks'}), 0.923076923076923]\n",
      "\n",
      "Rule 3: [frozenset({'Socks'}), frozenset({'Sweatshirts'}), 0.923076923076923]\n",
      "\n",
      "Rule 4: [frozenset({'Socks'}), frozenset({'Running Shoe'}), 0.8461538461538461]\n",
      "\n",
      "Rule 5: [frozenset({'Running Shoe', 'Sweatshirts'}), frozenset({'Socks'}), 0.9090909090909091]\n",
      "\n",
      "Rule 6: [frozenset({'Running Shoe', 'Socks'}), frozenset({'Sweatshirts'}), 0.9090909090909091]\n",
      "\n",
      "Rule 7: [frozenset({'Sweatshirts', 'Socks'}), frozenset({'Running Shoe'}), 0.8333333333333334]\n",
      "\n",
      "Rule 8: [frozenset({'Modern Pants'}), frozenset({'Sweatshirts'}), 1.0]\n",
      "\n",
      "Rule 9: [frozenset({'Rash Guard'}), frozenset({'Swimming Shirt'}), 0.8333333333333334]\n",
      "\n",
      "Rule 10: [frozenset({'Swimming Shirt'}), frozenset({'Rash Guard'}), 0.9090909090909091]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in fpgrowthRules.iterrows():\n",
    "    antecedent_str = row['antecedents']\n",
    "    consequent_str = row['consequents']\n",
    "    confidence_str = row['confidence']\n",
    "    \n",
    "    print(f\"Rule {index + 1}: [{antecedent_str}, {consequent_str}, {confidence_str}]\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
